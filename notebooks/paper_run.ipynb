{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58daa574-d78c-49c0-9779-dea90dd6a0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install kagglehub\n",
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5e7a3-b74a-4ea4-b6e6-3bf3ed6af301",
   "metadata": {},
   "source": [
    "### Get the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae069264-73d8-4258-98a9-a6eb1f470d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"chiranjivdas09/ta-feng-grocery-dataset\")\n",
    "print(\"Dataset path:\", path)\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(f\"DATASET_PATH={path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e99d9-a01d-4212-8c97-95b1464986cc",
   "metadata": {},
   "source": [
    "### Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595255e-049b-4997-b66b-d3251438c4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "path = os.getenv(\"DATASET_PATH\")\n",
    "\n",
    "if not path:\n",
    "    print(\"FAILED TO DOWNLOAD DATASET ‚Äî DATASET_PATH not found or empty.\")\n",
    "else:\n",
    "    print(\"Dataset path loaded successfully:\")\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c40e91-e8bd-4910-b9fd-73fd87113c95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### üìä EDA ‚Äî What this section does\n",
    "\n",
    "**Goal.** Understand the Ta-Feng dataset‚Äôs shape, data quality, and basic patterns to choose sensible preprocessing and mining settings.\n",
    "\n",
    "**Inputs.**\n",
    "- `ta_feng_all_months_merged.csv` (9 columns): `TRANSACTION_DT, CUSTOMER_ID, AGE_GROUP, PIN_CODE, PRODUCT_SUBCLASS, PRODUCT_ID, AMOUNT, ASSET, SALES_PRICE`\n",
    "\n",
    "**What happens (step-by-step).**\n",
    "1. **Load & standardize columns** ‚Äî Uppercase names; coerce types (IDs ‚Üí strings; `AMOUNT/ASSET/SALES_PRICE` ‚Üí numeric; `TRANSACTION_DT` ‚Üí datetime).\n",
    "2. **Coverage checks** ‚Äî Print min/max date, null counts, and basic stats for numeric columns.\n",
    "3. **Temporal features for inspection** ‚Äî Derive `YEAR_MONTH`, `WEEK`, `DAY_OF_WEEK` (for slicing/plots only).\n",
    "4. **Top-K distributions** ‚Äî Frequency tables for `PRODUCT_ID`, `PRODUCT_SUBCLASS`, `AGE_GROUP`, `PIN_CODE`, `YEAR_MONTH`, `DAY_OF_WEEK`.\n",
    "5. **Skew-aware histograms** ‚Äî For `AMOUNT`, `ASSET`, `SALES_PRICE`, show **Full** vs **Clipped @ 99th percentile** histograms side-by-side to make long tails interpretable.\n",
    "6. **Basket construction** ‚Äî Group lines into **transactions** by (`CUSTOMER_ID`, `TRANSACTION_DT`); collect unique item lists per transaction (`items`) and compute `basket_size`.\n",
    "7. **Co-occurrence peek** ‚Äî Top item pairs by co-appearance (sanity check before true itemset mining).\n",
    "8. **Temporal volume** ‚Äî Bar plots for transactions per week/month (seasonality/holidays).\n",
    "9. **Cache artifacts**  \n",
    "   - `artifacts/eda/transactions_normalized.parquet` ‚Äî clean, typed transactions  \n",
    "   - `artifacts/eda/baskets.parquet` ‚Äî per-transaction `items` + `basket_size`\n",
    "\n",
    "**Why this matters.**\n",
    "- Confirms time horizon & sparsity.\n",
    "- Reveals heavy-tail behavior (justifies winsorization later).\n",
    "- Validates basket construction (essential for frequent itemset mining).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5cd231-ec8b-4ea2-8204-bf56cbbf736b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === EDA for Ta-Feng (exact 9-column schema) ===\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "\n",
    "DATASET_DIR = Path(os.getenv(\"DATASET_PATH\") or \".\").resolve()\n",
    "CSV_PATH = None\n",
    "# Prefer the merged file; fall back to first CSV if path varies\n",
    "candidates = list(DATASET_DIR.rglob(\"ta_feng_all_months_merged.csv\"))\n",
    "if candidates:\n",
    "    CSV_PATH = candidates[0]\n",
    "else:\n",
    "    all_csvs = list(DATASET_DIR.rglob(\"*.csv\"))\n",
    "    if not all_csvs:\n",
    "        raise FileNotFoundError(f\"No CSVs found beneath: {DATASET_DIR}\")\n",
    "    # heuristically pick the largest csv (often the merged)\n",
    "    CSV_PATH = max(all_csvs, key=lambda p: p.stat().st_size)\n",
    "\n",
    "print(\"Using CSV:\", CSV_PATH)\n",
    "\n",
    "# 1) Load & normalize column names to snake_case\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "orig_cols = list(df.columns)\n",
    "df.columns = [c.strip().upper() for c in df.columns]\n",
    "\n",
    "expected = [\n",
    "    \"TRANSACTION_DT\",\"CUSTOMER_ID\",\"AGE_GROUP\",\"PIN_CODE\",\n",
    "    \"PRODUCT_SUBCLASS\",\"PRODUCT_ID\",\"AMOUNT\",\"ASSET\",\"SALES_PRICE\"\n",
    "]\n",
    "missing_expected = [c for c in expected if c not in df.columns]\n",
    "if missing_expected:\n",
    "    print(\"WARNING: missing expected columns:\", missing_expected)\n",
    "\n",
    "# Keep only the 9 we care about (if extras exist)\n",
    "cols = [c for c in expected if c in df.columns]\n",
    "df = df[cols].copy()\n",
    "\n",
    "# 2) Dtypes: IDs as strings; numeric as numeric; parse datetime\n",
    "# TRANSACTION_DT can be like '11/01/2000' or '1Nov00'‚Äîuse coerce\n",
    "def parse_dt(s):\n",
    "    return pd.to_datetime(s, errors=\"coerce\", dayfirst=False, infer_datetime_format=True)\n",
    "\n",
    "df[\"TRANSACTION_DT\"] = parse_dt(df[\"TRANSACTION_DT\"])\n",
    "\n",
    "for c in [\"CUSTOMER_ID\",\"PRODUCT_SUBCLASS\",\"PRODUCT_ID\",\"PIN_CODE\",\"AGE_GROUP\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(str)\n",
    "\n",
    "for c in [\"AMOUNT\",\"ASSET\",\"SALES_PRICE\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\n=== Schema preview ===\")\n",
    "print(df.head(3))\n",
    "print(\"\\nInfo:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nMissing values (top 20):\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "num_cols = [c for c in [\"AMOUNT\",\"ASSET\",\"SALES_PRICE\"] if c in df.columns]\n",
    "if num_cols:\n",
    "    print(\"\\nBasic numeric stats:\")\n",
    "    display(df[num_cols].describe(percentiles=[.25,.5,.75,.9,.95,.99]).T)\n",
    "\n",
    "# 3) Temporal enrichments\n",
    "df[\"YEAR_MONTH\"] = df[\"TRANSACTION_DT\"].dt.to_period(\"M\").astype(str)\n",
    "df[\"WEEK\"] = df[\"TRANSACTION_DT\"].dt.to_period(\"W\").astype(str)\n",
    "df[\"DAY_OF_WEEK\"] = df[\"TRANSACTION_DT\"].dt.day_name()\n",
    "\n",
    "print(\"\\nTemporal coverage:\")\n",
    "print(\"min date:\", df[\"TRANSACTION_DT\"].min())\n",
    "print(\"max date:\", df[\"TRANSACTION_DT\"].max())\n",
    "\n",
    "# 4) Quick distributions / counts\n",
    "def top_counts(col, k=10):\n",
    "    if col in df.columns:\n",
    "        vc = df[col].value_counts().head(k)\n",
    "        print(f\"\\nTop {k} {col}:\")\n",
    "        display(vc)\n",
    "\n",
    "top_counts(\"PRODUCT_ID\")\n",
    "top_counts(\"PRODUCT_SUBCLASS\")\n",
    "top_counts(\"AGE_GROUP\")\n",
    "top_counts(\"PIN_CODE\")\n",
    "top_counts(\"YEAR_MONTH\")\n",
    "top_counts(\"DAY_OF_WEEK\")\n",
    "\n",
    "# 5) Histograms for numeric fields: NORMAL vs CLIPPED side-by-side\n",
    "def plot_normal_and_clipped(series, title, bins=50, upper_q=0.99):\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if len(s) == 0:\n",
    "        print(f\"Skipping {title}: no data.\")\n",
    "        return\n",
    "    q_hi = s.quantile(upper_q)\n",
    "    s_clip = s.clip(upper=q_hi)\n",
    "    n_clipped = int((s > q_hi).sum())\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].hist(s, bins=bins)\n",
    "    axes[0].set_title(f\"{title} ‚Äî Full\")\n",
    "    axes[0].set_xlabel(title); axes[0].set_ylabel(\"freq\")\n",
    "\n",
    "    axes[1].hist(s_clip, bins=bins)\n",
    "    axes[1].set_title(f\"{title} ‚Äî Clipped ‚â§ {upper_q:.0%} (>{n_clipped} clipped)\")\n",
    "    axes[1].set_xlabel(title); axes[1].set_ylabel(\"freq\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for c in num_cols:\n",
    "    plot_normal_and_clipped(df[c], c, bins=50, upper_q=0.99)\n",
    "\n",
    "# 6) Basket construction:\n",
    "# Use (CUSTOMER_ID, TRANSACTION_DT) as transaction key.\n",
    "# If times are date-only (no hours), this still groups per customer-day (or per customer-timestamp if present).\n",
    "key_cols = [\"CUSTOMER_ID\", \"TRANSACTION_DT\"] if \"CUSTOMER_ID\" in df.columns else [\"TRANSACTION_DT\"]\n",
    "\n",
    "# Choose granularity: product_id (fine) vs product_subclass (coarser)\n",
    "# Force subclass-level items (coarser, more interpretable)\n",
    "# ITEM_COL = \"PRODUCT_SUBCLASS\"\n",
    "ITEM_COL = \"PRODUCT_ID\" if \"PRODUCT_ID\" in df.columns else \"PRODUCT_SUBCLASS\"\n",
    "baskets = (df.dropna(subset=[ITEM_COL])\n",
    "             .groupby(key_cols)[ITEM_COL]\n",
    "             .apply(lambda s: list(pd.unique(s.astype(str))))\n",
    "             .reset_index(name=\"items\"))\n",
    "\n",
    "print(\"\\nBaskets preview:\")\n",
    "print(baskets.head(5))\n",
    "print(\"Num baskets:\", len(baskets))\n",
    "\n",
    "# Basket size\n",
    "baskets[\"basket_size\"] = baskets[\"items\"].apply(len)\n",
    "baskets[\"basket_size\"].plot(kind=\"hist\", bins=30, title=\"Basket size distribution\")\n",
    "plt.xlabel(\"distinct items per transaction\"); plt.ylabel(\"freq\"); plt.show()\n",
    "\n",
    "print(\"\\nBasket size stats:\")\n",
    "display(baskets[\"basket_size\"].describe(percentiles=[.5,.75,.9,.95,.99]))\n",
    "\n",
    "# 7) Sanity checks: duplicate transactions, extreme outliers\n",
    "dups = baskets.duplicated(subset=key_cols).sum()\n",
    "print(\"\\nPotential duplicate transaction keys:\", int(dups))\n",
    "\n",
    "for c in num_cols:\n",
    "    q99 = df[c].quantile(0.99)\n",
    "    mx = df[c].max()\n",
    "    print(f\"{c}: 99th pct={q99:.2f}, max={mx:.2f}\")\n",
    "\n",
    "# 8) Quick co-occurrence peek (cap for speed)\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "pair_counts = Counter()\n",
    "for items in baskets[\"items\"].tolist()[:200000]:  # cap to keep it quick\n",
    "    s = sorted(set(items))\n",
    "    for a, b in combinations(s, 2):\n",
    "        pair_counts[(a,b)] += 1\n",
    "\n",
    "top = pd.DataFrame([(a,b,c) for (a,b),c in pair_counts.most_common(20)],\n",
    "                   columns=[f\"{ITEM_COL}_1\", f\"{ITEM_COL}_2\", \"pair_count\"])\n",
    "print(\"\\nTop co-occurring pairs:\")\n",
    "display(top)\n",
    "\n",
    "# 9) Weekly and monthly basket counts\n",
    "wk = baskets.copy()\n",
    "wk[\"WEEK\"] = wk[\"TRANSACTION_DT\"].dt.to_period(\"W\").astype(str)\n",
    "wk_counts = wk[\"WEEK\"].value_counts().sort_index()\n",
    "wk_counts.plot(kind=\"bar\", title=\"Transactions per week\"); plt.xlabel(\"week\"); plt.ylabel(\"#transactions\"); plt.show()\n",
    "\n",
    "mo_counts = baskets[\"TRANSACTION_DT\"].dt.to_period(\"M\").astype(str).value_counts().sort_index()\n",
    "mo_counts.plot(kind=\"bar\", title=\"Transactions per month\"); plt.xlabel(\"year-month\"); plt.ylabel(\"#transactions\"); plt.show()\n",
    "\n",
    "# 10) Cache artifacts for downstream steps\n",
    "OUT_DIR = Path(\"artifacts/eda\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Tidy normalized transactions (snake_case, standard dtypes)\n",
    "df_out = df.rename(columns={\n",
    "    \"TRANSACTION_DT\":\"transaction_dt\",\n",
    "    \"CUSTOMER_ID\":\"customer_id\",\n",
    "    \"AGE_GROUP\":\"age_group\",\n",
    "    \"PIN_CODE\":\"pin_code\",\n",
    "    \"PRODUCT_SUBCLASS\":\"product_subclass\",\n",
    "    \"PRODUCT_ID\":\"product_id\",\n",
    "    \"AMOUNT\":\"amount\",\n",
    "    \"ASSET\":\"asset\",\n",
    "    \"SALES_PRICE\":\"sales_price\",\n",
    "    \"YEAR_MONTH\":\"year_month\",\n",
    "    \"WEEK\":\"week\",\n",
    "    \"DAY_OF_WEEK\":\"day_of_week\",\n",
    "})\n",
    "\n",
    "df_out_path = OUT_DIR / \"transactions_normalized.parquet\"\n",
    "baskets_path = OUT_DIR / \"baskets.parquet\"\n",
    "df_out.to_parquet(df_out_path, index=False)\n",
    "baskets.to_parquet(baskets_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved normalized transactions ‚Üí {df_out_path}\")\n",
    "print(f\"Saved baskets ‚Üí {baskets_path}\")\n",
    "\n",
    "print(\"\"\"\n",
    "Report talking points to fill (after inspecting outputs):\n",
    "‚Ä¢ Temporal: confirm coverage (2000-11-01 to 2001-02-28), weekly peaks (e.g., holidays), and month-to-month shifts.\n",
    "‚Ä¢ Product mix: head vs long tail (PRODUCT_ID / PRODUCT_SUBCLASS), any notable families.\n",
    "‚Ä¢ Demographics: AGE_GROUP and PIN_CODE distributions; % missing in AGE_GROUP (~3%).\n",
    "‚Ä¢ Sales metrics: AMOUNT/ASSET/SALES_PRICE distributions; identify extreme outliers (list 99th pct vs max).\n",
    "‚Ä¢ Basket structure: typical/median size; tail behavior (95th/99th percentiles); implications for itemset mining.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa9a3c-9d9a-44e9-86c9-4e53186989e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### üßπ Data Preprocessing ‚Äî What this section does\n",
    "\n",
    "**Goal.** Turn raw transactions into **clean, model-ready features** for clustering/dimensionality reduction and downstream mining, while keeping a minimally imputed facts table.\n",
    "\n",
    "**Inputs.**\n",
    "- `artifacts/eda/transactions_normalized.parquet`  \n",
    "- `artifacts/eda/baskets.parquet`\n",
    "\n",
    "**What happens (step-by-step).**\n",
    "1. **Missing-data handling**\n",
    "   - Drop rows missing transaction keys (`transaction_dt`, and `customer_id` if present).\n",
    "   - Impute categoricals: `AGE_GROUP`/`PIN_CODE` ‚Üí `\"Unknown\"`.\n",
    "   - Keep numeric NaNs for now; median-impute later inside pipelines.\n",
    "2. **Feature engineering (transaction-level)**\n",
    "   - Aggregate per transaction: `total_amount`, `total_asset`, `total_sales`, `n_lines` (row count), `basket_size` (distinct items), `avg_unit_price = total_sales / total_amount`.\n",
    "   - Attach lightweight context: `year_month`, `day_of_week`, `week`, plus demographics if present.\n",
    "3. **Feature engineering (customer-level)**\n",
    "   - RFM-like signals: `txn_count`, `days_active`, `total_sales_sum/mean/median`, `avg_basket_size`, `avg_unit_price`, `recency_days`.\n",
    "   - Compact preference profile: **Top-K (K=10) `product_subclass` proportions** per customer.\n",
    "4. **Scaling & encoding (modeling matrices)**\n",
    "   - **Winsorize** numeric features at **99.5th percentile** (modeling copies only) to tame long tails.\n",
    "   - **Numerics:** median **imputation** ‚Üí **StandardScaler** (zero-mean/unit-variance).\n",
    "   - **Categoricals:** **One-Hot Encoding** with `handle_unknown='ignore'` (e.g., `age_group`, `pin_code`, `day_of_week`, `year_month` for transactions).\n",
    "   - Outputs:\n",
    "     - `X_txn.npy` + `X_txn_feature_names.json` ‚Äî transaction-level matrix & names\n",
    "     - `X_cust.npy` + `X_cust_feature_names.json` ‚Äî customer-level matrix & names\n",
    "5. **Save tidy references**\n",
    "   - `artifacts/preprocessed/transactions_clean.parquet` ‚Äî minimally imputed facts  \n",
    "   - `artifacts/preprocessed/transactions_features.parquet` ‚Äî transaction features  \n",
    "   - `artifacts/preprocessed/customers_features.parquet` ‚Äî customer features (if applicable)\n",
    "\n",
    "**Why this matters.**\n",
    "- EDA showed heavy skew & high cardinality; this pipeline stabilizes features for distance-based methods (k-means, PCA/UMAP) and preserves interpretable copies.\n",
    "- **Top-K subclass proportions** capture product-mix signals without exploding dimensionality.\n",
    "\n",
    "**Knobs you can tweak.**\n",
    "- Winsorization level (`upper_q=0.995`) for tail strength.\n",
    "- Item granularity in EDA (`PRODUCT_ID` vs `PRODUCT_SUBCLASS`) for basket construction.\n",
    "- K for subclass proportions (default 10) based on variance explained or downstream performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f7189-6c4c-498d-a987-cdd895f776f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Data Preprocessing: missing handling, feature engineering, scaling (FIXED) ===\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "ART_DIR = Path(\"artifacts\")\n",
    "EDA_DIR = ART_DIR / \"eda\"\n",
    "OUT_DIR = ART_DIR / \"preprocessed\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load normalized transactions + baskets from EDA\n",
    "df = pd.read_parquet(EDA_DIR / \"transactions_normalized.parquet\")\n",
    "baskets = pd.read_parquet(EDA_DIR / \"baskets.parquet\")\n",
    "\n",
    "# --- KEY FIX: normalize baskets column names & ensure basket_size exists ---\n",
    "baskets = baskets.copy()\n",
    "baskets.columns = [c.strip().lower() for c in baskets.columns]\n",
    "\n",
    "# Ensure 'items' exists\n",
    "assert \"items\" in baskets.columns, \"baskets.parquet must contain an 'items' column.\"\n",
    "\n",
    "# Add basket_size if absent\n",
    "if \"basket_size\" not in baskets.columns:\n",
    "    baskets[\"basket_size\"] = baskets[\"items\"].apply(lambda xs: len(set(xs)) if isinstance(xs, (list, tuple)) else 0)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Handling Missing Data\n",
    "# -----------------------------\n",
    "# Critical keys should not be missing\n",
    "key_cols = [\"transaction_dt\"]\n",
    "if \"customer_id\" in df.columns:\n",
    "    key_cols.append(\"customer_id\")\n",
    "\n",
    "df = df.dropna(subset=key_cols).copy()\n",
    "\n",
    "# AGE_GROUP: impute to 'Unknown' (~3% missing)\n",
    "if \"age_group\" in df.columns:\n",
    "    df[\"age_group\"] = df[\"age_group\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "# PIN_CODE: fill blanks with 'Unknown'\n",
    "if \"pin_code\" in df.columns:\n",
    "    df[\"pin_code\"] = df[\"pin_code\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "# Numeric coercion (re-assert)\n",
    "for c in [\"amount\", \"asset\", \"sales_price\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Feature Engineering\n",
    "# -----------------------------\n",
    "# Temporal features\n",
    "df[\"year_month\"] = df[\"transaction_dt\"].dt.to_period(\"M\").astype(str)\n",
    "df[\"day_of_week\"] = df[\"transaction_dt\"].dt.day_name()\n",
    "df[\"week\"] = df[\"transaction_dt\"].dt.to_period(\"W\").astype(str)\n",
    "\n",
    "# Transaction-level totals (sum over lines)\n",
    "txn_key = [\"customer_id\", \"transaction_dt\"] if \"customer_id\" in df.columns else [\"transaction_dt\"]\n",
    "\n",
    "agg_txn = df.groupby(txn_key).agg(\n",
    "    total_amount=(\"amount\", \"sum\"),\n",
    "    total_asset=(\"asset\", \"sum\"),\n",
    "    total_sales=(\"sales_price\", \"sum\"),\n",
    "    n_lines=(\"product_id\", \"count\") if \"product_id\" in df.columns else (\"product_subclass\", \"count\"),\n",
    ").reset_index()\n",
    "\n",
    "# Merge basket_size from baskets (distinct items)\n",
    "# Align keys in baskets to match df (already lowercase)\n",
    "needed_keys = [k for k in txn_key if k in baskets.columns]\n",
    "assert set(needed_keys) == set(txn_key), (\n",
    "    f\"Key mismatch between df {txn_key} and baskets {list(baskets.columns)}. \"\n",
    "    \"Ensure baskets has the same key columns.\"\n",
    ")\n",
    "\n",
    "tmp = baskets[txn_key + [\"basket_size\"]].copy()\n",
    "agg_txn = agg_txn.merge(tmp, on=txn_key, how=\"left\")\n",
    "\n",
    "# Unit price proxy\n",
    "agg_txn[\"avg_unit_price\"] = np.where(agg_txn[\"total_amount\"] > 0,\n",
    "                                     agg_txn[\"total_sales\"] / agg_txn[\"total_amount\"],\n",
    "                                     np.nan)\n",
    "\n",
    "# Attach simple categorical/time context to transactions\n",
    "ctx_cols = [\"age_group\",\"pin_code\",\"year_month\",\"day_of_week\",\"week\"]\n",
    "ctx_merge = (df[txn_key + [c for c in ctx_cols if c in df.columns]]\n",
    "               .drop_duplicates(subset=txn_key))\n",
    "agg_txn = agg_txn.merge(ctx_merge, on=txn_key, how=\"left\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2b) Customer-level aggregates\n",
    "# -----------------------------\n",
    "cust_key = [\"customer_id\"] if \"customer_id\" in df.columns else None\n",
    "if cust_key:\n",
    "    cust = agg_txn.groupby(\"customer_id\").agg(\n",
    "        txn_count=(\"total_sales\", \"size\"),\n",
    "        days_active=(\"transaction_dt\", lambda s: (s.max() - s.min()).days if s.notna().any() else 0),\n",
    "        total_sales_sum=(\"total_sales\", \"sum\"),\n",
    "        total_sales_mean=(\"total_sales\", \"mean\"),\n",
    "        total_sales_median=(\"total_sales\", \"median\"),\n",
    "        avg_basket_size=(\"basket_size\", \"mean\"),\n",
    "        avg_unit_price=(\"avg_unit_price\", \"mean\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # Recency proxy\n",
    "    end_date = df[\"transaction_dt\"].max()\n",
    "    last_txn = agg_txn.groupby(\"customer_id\")[\"transaction_dt\"].max().reset_index(name=\"last_txn_dt\")\n",
    "    last_txn[\"recency_days\"] = (end_date - last_txn[\"last_txn_dt\"]).dt.days\n",
    "    cust = cust.merge(last_txn[[\"customer_id\",\"recency_days\"]], on=\"customer_id\", how=\"left\")\n",
    "\n",
    "    # Subclass composition (top-K proportions)  ‚úÖ FIXED (use transform, not apply)\n",
    "    K = 10\n",
    "    if \"product_subclass\" in df.columns:\n",
    "        sub_pivot = (\n",
    "            df.groupby([\"customer_id\", \"product_subclass\"])\n",
    "              .size()\n",
    "              .reset_index(name=\"cnt\")\n",
    "        )\n",
    "        # per-customer totals with transform keeps the original index ‚Üí safe to assign\n",
    "        totals = sub_pivot.groupby(\"customer_id\")[\"cnt\"].transform(\"sum\").replace(0, np.nan)\n",
    "        sub_pivot[\"prop\"] = sub_pivot[\"cnt\"] / totals\n",
    "\n",
    "        # pick global top-K subclasses to limit width\n",
    "        global_topK = (\n",
    "            sub_pivot.groupby(\"product_subclass\")[\"cnt\"].sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(K)\n",
    "            .index.tolist()\n",
    "        )\n",
    "\n",
    "        sub_top = (\n",
    "            sub_pivot[sub_pivot[\"product_subclass\"].isin(global_topK)]\n",
    "            .pivot(index=\"customer_id\", columns=\"product_subclass\", values=\"prop\")\n",
    "            .fillna(0.0)\n",
    "        )\n",
    "        sub_top.columns = [f\"subcls_prop_{c}\" for c in sub_top.columns]\n",
    "        cust = (\n",
    "            cust.merge(sub_top, left_on=\"customer_id\", right_index=True, how=\"left\")\n",
    "                .fillna(0.0)\n",
    "        )\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Standardisation / Normalisation\n",
    "# -----------------------------\n",
    "def winsorize_series(s, upper_q=0.995):\n",
    "    q = s.quantile(upper_q)\n",
    "    return s.clip(upper=q)\n",
    "\n",
    "WINSORIZE_FOR_MODELING = True\n",
    "NUM_TXN = [\"total_amount\",\"total_asset\",\"total_sales\",\"basket_size\",\"n_lines\",\"avg_unit_price\"]\n",
    "NUM_TXN = [c for c in NUM_TXN if c in agg_txn.columns]\n",
    "CAT_TXN = [c for c in [\"age_group\",\"pin_code\",\"day_of_week\",\"year_month\"] if c in agg_txn.columns]\n",
    "\n",
    "if WINSORIZE_FOR_MODELING:\n",
    "    for c in NUM_TXN:\n",
    "        agg_txn[c] = winsorize_series(agg_txn[c])\n",
    "\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "txn_ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline(steps=[(\"impute\", num_imputer), (\"scale\", StandardScaler())]),\n",
    "         NUM_TXN),\n",
    "        (\"cat\", Pipeline(steps=[(\"impute\", cat_imputer), (\"ohe\", ohe)]),\n",
    "         CAT_TXN),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "X_txn = txn_ct.fit_transform(agg_txn)\n",
    "num_names = NUM_TXN\n",
    "cat_names = list(txn_ct.named_transformers_[\"cat\"].named_steps[\"ohe\"].get_feature_names_out(CAT_TXN)) if CAT_TXN else []\n",
    "txn_feature_names = num_names + cat_names\n",
    "\n",
    "np.save(OUT_DIR / \"X_txn.npy\", X_txn)\n",
    "pd.DataFrame(agg_txn[txn_key + NUM_TXN + CAT_TXN]).to_parquet(OUT_DIR / \"transactions_features.parquet\", index=False)\n",
    "with open(OUT_DIR / \"X_txn_feature_names.json\", \"w\") as f:\n",
    "    import json; json.dump(txn_feature_names, f, indent=2)\n",
    "\n",
    "print(f\"Transaction-level matrix: X_txn shape = {X_txn.shape}\")\n",
    "\n",
    "# Customer-level matrix\n",
    "if cust_key:\n",
    "    NUM_CUST = [\"txn_count\",\"days_active\",\"total_sales_sum\",\"total_sales_mean\",\"total_sales_median\",\n",
    "                \"avg_basket_size\",\"avg_unit_price\",\"recency_days\"]\n",
    "    NUM_CUST += [c for c in cust.columns if c.startswith(\"subcls_prop_\")]\n",
    "    if WINSORIZE_FOR_MODELING:\n",
    "        for c in NUM_CUST:\n",
    "            cust[c] = winsorize_series(cust[c])\n",
    "\n",
    "    cust_ct = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline(steps=[(\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "                                   (\"scale\", StandardScaler())]),\n",
    "             NUM_CUST)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    X_cust = cust_ct.fit_transform(cust)\n",
    "    np.save(OUT_DIR / \"X_cust.npy\", X_cust)\n",
    "    cust.to_parquet(OUT_DIR / \"customers_features.parquet\", index=False)\n",
    "    with open(OUT_DIR / \"X_cust_feature_names.json\", \"w\") as f:\n",
    "        import json; json.dump(NUM_CUST, f, indent=2)\n",
    "    print(f\"Customer-level matrix: X_cust shape = {X_cust.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Save cleaned transactions (minimal imputation)\n",
    "# -----------------------------\n",
    "df.to_parquet(OUT_DIR / \"transactions_clean.parquet\", index=False)\n",
    "\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(f\"- {OUT_DIR/'transactions_clean.parquet'}\")\n",
    "print(f\"- {OUT_DIR/'transactions_features.parquet'}\")\n",
    "if cust_key:\n",
    "    print(f\"- {OUT_DIR/'customers_features.parquet'}\")\n",
    "print(f\"- {OUT_DIR/'X_txn.npy'} (+ X_txn_feature_names.json)\")\n",
    "if cust_key:\n",
    "    print(f\"- {OUT_DIR/'X_cust.npy'} (+ X_cust_feature_names.json)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582e4fc-47f9-4d60-a558-017520986c98",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Data Mining Methods and Analysis\n",
    "\n",
    "**Setting.** Each transaction is a set of items with **existential probabilities** \\(P_t(x)\\in(0,1]\\). We mine frequent itemsets under **expected support**:\n",
    "\\[\n",
    "S_e(X)=\\sum_{t\\in D}\\ \\prod_{x\\in X} P_t(x),\n",
    "\\]\n",
    "and declare \\(X\\) frequent if \\(S_e(X)\\ge \\theta\\) (absolute minsup).\n",
    "\n",
    "**Algorithms compared.**\n",
    "1. **U-Apriori (Expected Support).** Level-wise candidate generation (Apriori-Gen) with expected-support counting (product of item probabilities within transactions).\n",
    "2. **LGS-Trimming** (paper‚Äôs speedup for uncertain data):\n",
    "   - **Local trimming:** per-item threshold \\(\\rho_t(x)\\) removes very small \\(P_t(x)\\) to form a trimmed dataset \\(D_T\\).\n",
    "   - **Global pruning:** compute an **upper bound** for the lost (trimmed-away) contribution; if \\(S_e^T(X)+\\text{UB}(X)<\\theta\\), prune.\n",
    "   - **Single-pass patch-up:** recompute exact expected support on the **original** (untrimmed) data for survivors and finalize frequency.\n",
    "\n",
    "**Protocol.**\n",
    "- Build a **single probabilistic dataset** from the baskets (jitter probabilities for observed items and add small-probability noise items) and keep it fixed.\n",
    "- Run **U-Apriori** and **LGS-Trimming** with the **same minsup**.\n",
    "- Report runtime, number of frequent itemsets, and top-k itemsets by expected support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8122261-c6ca-4f80-8a61-bb33abff3959",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Config + probabilistic data (single source of truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67e382-7251-4bfe-a311-da7e163864e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Probabilistic data per paper: high/low Gaussians + R control ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "EDA_DIR = Path(\"artifacts/eda\")\n",
    "baskets = pd.read_parquet(EDA_DIR / \"baskets.parquet\").copy()\n",
    "assert \"items\" in baskets, \"baskets.parquet must contain an 'items' column.\"\n",
    "N_TX = len(baskets)\n",
    "\n",
    "# === Experiment knobs (tune to mirror the paper's sweeps) ===\n",
    "REL_MINSUP = 0.005              # e.g., 0.5%  (you can sweep later: 1%, 0.5%, 0.2%, 0.1%)\n",
    "ABS_MINSUP = REL_MINSUP * N_TX\n",
    "TOPK_SHOW  = 20\n",
    "\n",
    "# R = T_low / (T_high + T_low) ‚Üí share of low-probability items in the dataset\n",
    "R = 0.333                        # try {0.0, 0.333, 0.5, 0.667, 0.75}\n",
    "\n",
    "# High/Low Gaussian params (means, stddevs); clip to [0,1]\n",
    "HB, HD = 0.90, 0.05             # \"high-prob\" ~ N(0.90, 0.05^2)\n",
    "LB, LD = 0.10, 0.05             # \"low-prob\"  ~ N(0.10, 0.05^2)\n",
    "\n",
    "# Optional: add a small number of *extra* low-prob items per transaction\n",
    "ADD_NOISE_LOW = False\n",
    "NOISE_K = 3                      # # of extra items to inject per transaction\n",
    "NOISE_P_IS_GAUSSIAN = True       # if True draw from N(LB, LD), else fixed LB\n",
    "\n",
    "SEED = 7\n",
    "rng_py = random.Random(SEED)\n",
    "rng_np = np.random.default_rng(SEED)\n",
    "\n",
    "ALL_ITEMS = sorted({it for xs in baskets[\"items\"] for it in set(xs)})\n",
    "\n",
    "def _clip01(x):\n",
    "    return float(np.clip(x, 0.0, 1.0))\n",
    "\n",
    "def _draw_high(n):\n",
    "    return np.clip(rng_np.normal(HB, HD, size=n), 0.0, 1.0).astype(float)\n",
    "\n",
    "def _draw_low(n):\n",
    "    return np.clip(rng_np.normal(LB, LD, size=n), 0.0, 1.0).astype(float)\n",
    "\n",
    "def make_probabilistic_R_model(baskets, R, add_noise_low=False, noise_k=5, noise_gaussian=True):\n",
    "    \"\"\"\n",
    "    Paper-style uncertainty:\n",
    "      - Start from deterministic baskets (observed items).\n",
    "      - Assign each observed item to 'low' with probability R and 'high' otherwise.\n",
    "        Draw p from N(HB,HD) for high, N(LB,LD) for low; clip to [0,1].\n",
    "      - Optionally add 'noise_k' extra low-probability items per transaction.\n",
    "\n",
    "    Notes:\n",
    "      - This achieves the target R approximately over the *observed* item occurrences.\n",
    "      - Adding extra low-prob noise per txn increases the global low share slightly; keep noise_k small.\n",
    "      - For exact global R matching, we could post-correct, but the paper's trends are robust to small deltas.\n",
    "    \"\"\"\n",
    "    prob_txns = []\n",
    "    low_count, total_count = 0, 0\n",
    "\n",
    "    for items in baskets[\"items\"]:\n",
    "        items = list(set(map(str, items)))\n",
    "        m = len(items)\n",
    "        # Bernoulli assignment of low/high for observed items\n",
    "        is_low = rng_np.random(m) < R\n",
    "        ps = np.empty(m, dtype=float)\n",
    "        # draw probs\n",
    "        n_low = int(is_low.sum())\n",
    "        n_high = m - n_low\n",
    "        if n_high > 0:\n",
    "            ps[~is_low] = _draw_high(n_high)\n",
    "        if n_low > 0:\n",
    "            ps[is_low] = _draw_low(n_low)\n",
    "\n",
    "        tx = {it: _clip01(p) for it, p in zip(items, ps)}\n",
    "        low_count += int(n_low); total_count += m\n",
    "\n",
    "        # Optional: add extra low-prob items (noise)\n",
    "        if add_noise_low and noise_k > 0:\n",
    "            # sample without replacing existing items\n",
    "            add_pool = [it for it in ALL_ITEMS if it not in tx]\n",
    "            for _ in range(min(noise_k, len(add_pool))):\n",
    "                it = rng_py.choice(add_pool)\n",
    "                add_pool.remove(it)\n",
    "                if noise_gaussian:\n",
    "                    p = float(_draw_low(1)[0])\n",
    "                else:\n",
    "                    p = float(LB)\n",
    "                tx[it] = _clip01(p)\n",
    "                low_count += 1; total_count += 1\n",
    "\n",
    "        prob_txns.append(tx)\n",
    "\n",
    "    achieved_R = low_count / max(1, total_count)\n",
    "    print(f\"Target R={R:.3f} | Achieved R‚âà{achieved_R:.3f} \"\n",
    "          f\"(low={low_count}, total={total_count})\")\n",
    "    return prob_txns\n",
    "\n",
    "# Build the canonical uncertain dataset for ALL methods (U-Apriori & LGS)\n",
    "prob_txns = make_probabilistic_R_model(\n",
    "    baskets,\n",
    "    R=R,\n",
    "    add_noise_low=ADD_NOISE_LOW,\n",
    "    noise_k=NOISE_K,\n",
    "    noise_gaussian=NOISE_P_IS_GAUSSIAN\n",
    ")\n",
    "\n",
    "print(f\"Transactions: {N_TX} | rel minsup={REL_MINSUP:.3%} | abs minsup={ABS_MINSUP:.2f}\")\n",
    "print(\"Example txn (first 5 items):\", list(prob_txns[0].items())[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec3bb3-c359-4a38-90ba-91f9372a88a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 U-Apriori (expected support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da03c2e-773c-4c47-9e5f-c44d8c2ef658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- U-Apriori (Expected Support) ---\n",
    "from itertools import combinations\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "\n",
    "def apriori_gen(Lk_1):\n",
    "    \"\"\"\n",
    "    Join frequent (k-1)-itemsets (as frozensets) to form size-k candidates (as frozensets).\n",
    "    Correctly checks that all (k-1)-subsets of a candidate are in Lk_1.\n",
    "    \"\"\"\n",
    "    # normalize: list of sorted tuples just for deterministic joining\n",
    "    L = sorted([tuple(sorted(x)) for x in Lk_1])\n",
    "    if not L:\n",
    "        return set()\n",
    "    k = len(L[0]) + 1\n",
    "    Ck = set()\n",
    "    for i in range(len(L)):\n",
    "        for j in range(i + 1, len(L)):\n",
    "            a, b = L[i], L[j]\n",
    "            # join on common prefix (length k-2)\n",
    "            if a[:-1] == b[:-1]:\n",
    "                cand = tuple(sorted(a + (b[-1],)))          # tuple for deterministic order\n",
    "                # ‚úÖ fix: check presence of all (k-1)-subsets in Lk_1 using frozenset\n",
    "                if all(frozenset(s) in Lk_1 for s in combinations(cand, k - 1)):\n",
    "                    Ck.add(frozenset(cand))                  # store as frozenset\n",
    "    return Ck\n",
    "\n",
    "\n",
    "def expected_support_k(prob_txns, Ck):\n",
    "    Se = {c: 0.0 for c in Ck}\n",
    "    for tx in prob_txns:\n",
    "        txset = set(tx.keys())\n",
    "        cand_here = [c for c in Ck if c.issubset(txset)]\n",
    "        for c in cand_here:\n",
    "            prod = 1.0\n",
    "            for it in c:\n",
    "                prod *= tx[it]\n",
    "            Se[c] += prod\n",
    "    return Se\n",
    "\n",
    "def u_apriori(prob_txns, abs_minsup):\n",
    "    t0 = time.time()\n",
    "    # 1-itemsets\n",
    "    Se1 = Counter()\n",
    "    for tx in prob_txns:\n",
    "        for it, p in tx.items():\n",
    "            Se1[frozenset([it])] += p\n",
    "    L1 = {c for c,se in Se1.items() if se >= abs_minsup}\n",
    "    Se_map = dict(Se1)\n",
    "    levels = [L1]\n",
    "\n",
    "    while levels[-1]:\n",
    "        Ck = apriori_gen(levels[-1])\n",
    "        if not Ck: break\n",
    "        Sek = expected_support_k(prob_txns, Ck)\n",
    "        Lk  = {c for c,se in Sek.items() if se >= abs_minsup}\n",
    "        Se_map.update(Sek)\n",
    "        levels.append(Lk)\n",
    "\n",
    "    F = set().union(*levels) if levels else set()\n",
    "    secs = time.time() - t0\n",
    "    return F, Se_map, secs\n",
    "\n",
    "F_u, Se_u, secs_u = u_apriori(prob_txns, ABS_MINSUP)\n",
    "print(f\"U-Apriori: |F|={len(F_u)} | time={secs_u:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff787eb-9584-4b38-abc7-07f479a83dee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 LGS-Trimming (local trim ‚Üí global prune ‚Üí patch-up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c476a6c-6407-460b-b46e-c84dec0736c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LGS-Trimming pipeline ---\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "def collect_item_probs(prob_txns):\n",
    "    obs = defaultdict(list)\n",
    "    for tx in prob_txns:\n",
    "        for it, p in tx.items():\n",
    "            obs[it].append(p)\n",
    "    return obs\n",
    "\n",
    "def choose_local_thresholds(item_obs, abs_minsup, early_frac=0.20):\n",
    "    \"\"\"\n",
    "    Local trimming thresholds œÅ_t(x) per item x.\n",
    "\n",
    "    For each item x:\n",
    "      1) Sort observed existential probabilities p_i in descending order.\n",
    "      2) Compute cumulative mass c_k = sum_{i<=k} p_i.\n",
    "      3) p1 (crossing): first index k where c_k >= abs_minsup; threshold = p_k.\n",
    "      4) p2 (knee): normalize x = k/n, y = c_k / c_n and choose k maximizing (y - x).\n",
    "         (This is a robust elbow/knee heuristic for concave cumulative curves.)\n",
    "      5) Decision:\n",
    "           - If crossing exists and happens \"early\" (k/n <= early_frac), use p1.\n",
    "           - Else use p2.\n",
    "    Edge cases:\n",
    "      - No observations: œÅ = 1.0 (effectively keep nothing).\n",
    "      - Flat/monotone mass (no knee): fallback to mid index or last prob > 0.\n",
    "    \"\"\"\n",
    "    rho_t = {}\n",
    "    for it, plist in item_obs.items():\n",
    "        if not plist:\n",
    "            rho_t[it] = 1.0\n",
    "            continue\n",
    "\n",
    "        # 1) sort probs descending\n",
    "        l = np.array(sorted(plist, reverse=True), dtype=float)\n",
    "        n = len(l)\n",
    "\n",
    "        # Guard: if all zeros, set œÅ high\n",
    "        if np.all(l <= 0):\n",
    "            rho_t[it] = 1.0\n",
    "            continue\n",
    "\n",
    "        # 2) cumulative mass\n",
    "        c = np.cumsum(l)\n",
    "        total = c[-1]\n",
    "\n",
    "        # 3) p1 = first crossing of abs_minsup, if any\n",
    "        p1_idx = None\n",
    "        if abs_minsup > 0:\n",
    "            hit = np.where(c >= abs_minsup)[0]\n",
    "            if len(hit) > 0:\n",
    "                p1_idx = int(hit[0])\n",
    "        p1_thr = l[p1_idx] if p1_idx is not None else None\n",
    "\n",
    "        # 4) p2 = knee via max(y - x) on normalized cumulative curve\n",
    "        #    x in [1/n, ..., 1], y in [c1/total, ..., 1]\n",
    "        #    (y - x) peaks at the elbow for concave curves.\n",
    "        x = (np.arange(1, n + 1) / n)\n",
    "        y = c / max(total, 1e-12)\n",
    "        diff = y - x\n",
    "        knee_idx = int(np.argmax(diff))  # robust elbow index\n",
    "        p2_thr = float(l[knee_idx])\n",
    "\n",
    "        # 5) decide regime\n",
    "        use_p1 = False\n",
    "        if p1_idx is not None:\n",
    "            frac = (p1_idx + 1) / n  # crossing position as a fraction\n",
    "            if frac <= early_frac:\n",
    "                use_p1 = True\n",
    "\n",
    "        rho_t[it] = float(p1_thr) if use_p1 and (p1_thr is not None) else float(p2_thr)\n",
    "\n",
    "        # Safety: ensure threshold ‚àà (0,1] and not greater than max observed\n",
    "        rho_t[it] = max(0.0, min(rho_t[it], float(l[0])))\n",
    "\n",
    "    return rho_t\n",
    "\n",
    "\n",
    "def build_trimmed_dataset(prob_txns, rho_t):\n",
    "    DT = []\n",
    "    ST_e, SnotT_e = defaultdict(float), defaultdict(float)\n",
    "    MT, MnotT     = defaultdict(float), defaultdict(float)\n",
    "    for tx in prob_txns:\n",
    "        kept = {}\n",
    "        for it, p in tx.items():\n",
    "            if p >= rho_t.get(it, 1.0):\n",
    "                kept[it] = p\n",
    "                ST_e[it] += p\n",
    "                MT[it] = max(MT[it], p)\n",
    "            else:\n",
    "                SnotT_e[it] += p\n",
    "                MnotT[it] = max(MnotT[it], p)\n",
    "        DT.append(kept)\n",
    "    return DT, dict(ST_e), dict(SnotT_e), dict(MT), dict(MnotT)\n",
    "\n",
    "def expected_support_k_on_DT(DT, Ck):\n",
    "    Se = {c: 0.0 for c in Ck}\n",
    "    for tx in DT:\n",
    "        txset = set(tx.keys())\n",
    "        cand_here = [c for c in Ck if c.issubset(txset)]\n",
    "        for c in cand_here:\n",
    "            prod = 1.0\n",
    "            for it in c:\n",
    "                prod *= tx[it]\n",
    "            Se[c] += prod\n",
    "    return Se\n",
    "\n",
    "def _safe_div(num, den):\n",
    "    return float(num) / float(den) if den and den > 0 else 0.0\n",
    "\n",
    "def _term_ST_notT_AB(A, B, ST_e_item, SnotT_e_item, MT_item, MnotT_item, ST_e_AB):\n",
    "    # Equation (5): ≈ú^{T,~T}_e(AB)\n",
    "    MT_A   = MT_item.get(A, 0.0)\n",
    "    MnotT_B= MnotT_item.get(B, 0.0)\n",
    "    if MT_A == 0 or MnotT_B == 0:\n",
    "        return 0.0\n",
    "    ST_e_A = ST_e_item.get(A, 0.0)\n",
    "    SnotT_e_B = SnotT_e_item.get(B, 0.0)\n",
    "    cap_left  = _safe_div( max(0.0, ST_e_A - ST_e_AB), MT_A )\n",
    "    cap_right = _safe_div( SnotT_e_B, MnotT_B )\n",
    "    return min(cap_left, cap_right) * MT_A * MnotT_B\n",
    "\n",
    "def _term_notT_ST_AB(A, B, ST_e_item, SnotT_e_item, MT_item, MnotT_item, ST_e_AB):\n",
    "    # Equation (6): ≈ú^{~T,T}_e(AB)\n",
    "    MT_B   = MT_item.get(B, 0.0)\n",
    "    MnotT_A= MnotT_item.get(A, 0.0)\n",
    "    if MT_B == 0 or MnotT_A == 0:\n",
    "        return 0.0\n",
    "    SnotT_e_A = SnotT_e_item.get(A, 0.0)\n",
    "    ST_e_B    = ST_e_item.get(B, 0.0)\n",
    "    cap_left  = _safe_div( SnotT_e_A, MnotT_A )\n",
    "    cap_right = _safe_div( max(0.0, ST_e_B - ST_e_AB), MT_B )\n",
    "    return min(cap_left, cap_right) * MnotT_A * MT_B\n",
    "\n",
    "def _term_notT_notT_AB(A, B, SnotT_e_item, MnotT_item, term_ST_notT, term_notT_ST):\n",
    "    # Equation (7): ≈ú^{~T,~T}_e(AB)\n",
    "    MnotT_A = MnotT_item.get(A, 0.0)\n",
    "    MnotT_B = MnotT_item.get(B, 0.0)\n",
    "    if MnotT_A == 0 or MnotT_B == 0:\n",
    "        return 0.0\n",
    "    SnotT_e_A = SnotT_e_item.get(A, 0.0)\n",
    "    SnotT_e_B = SnotT_e_item.get(B, 0.0)\n",
    "    cap_left  = _safe_div( max(0.0, SnotT_e_A - term_notT_ST), MnotT_A )\n",
    "    cap_right = _safe_div( max(0.0, SnotT_e_B - term_ST_notT), MnotT_B )\n",
    "    return min(cap_left, cap_right) * MnotT_A * MnotT_B\n",
    "\n",
    "def global_prune_upper_bound_pair(X, ST_e_item, SnotT_e_item, MT_item, MnotT_item, ST_e_AB):\n",
    "    \"\"\"\n",
    "    Exact global error bound for pairs {A,B} per the paper:\n",
    "      e(AB) = ≈ú^{T,~T}_e + ≈ú^{~T,T}_e + ≈ú^{~T,~T}_e\n",
    "    Uses Equations (5), (6), (7). If |X| != 2, returns None (caller can fallback).\n",
    "    \"\"\"\n",
    "    if len(X) != 2:\n",
    "        return None\n",
    "    A, B = tuple(sorted(X))\n",
    "    term1 = _term_ST_notT_AB(A, B, ST_e_item, SnotT_e_item, MT_item, MnotT_item, ST_e_AB)\n",
    "    term2 = _term_notT_ST_AB(A, B, ST_e_item, SnotT_e_item, MT_item, MnotT_item, ST_e_AB)\n",
    "    term3 = _term_notT_notT_AB(A, B, SnotT_e_item, MnotT_item, term1, term2)\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "\n",
    "def lgs_trimming(prob_txns, abs_minsup):\n",
    "    import time\n",
    "    from collections import defaultdict, Counter\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # --- Local thresholds & trimmed dataset ---\n",
    "    item_obs = collect_item_probs(prob_txns)\n",
    "    rho_t    = choose_local_thresholds(item_obs, abs_minsup)\n",
    "    DT, ST_e, SnotT_e, MT, MnotT = build_trimmed_dataset(prob_txns, rho_t)\n",
    "\n",
    "    # --- k=1 on DT ---\n",
    "    Se1 = Counter()\n",
    "    for tx in DT:\n",
    "        for it, p in tx.items():\n",
    "            Se1[frozenset([it])] += p\n",
    "\n",
    "    # Items frequent on DT (used for joining)\n",
    "    L1_DT = {c for c, se in Se1.items() if se >= abs_minsup}\n",
    "\n",
    "    # ‚úÖ Singletons that are infrequent on DT but frequent overall.\n",
    "    # These are for FINAL OUTPUT ONLY, not for joining.\n",
    "    L1_extra_output_only = set()\n",
    "    all_items = set(ST_e.keys()) | set(SnotT_e.keys())\n",
    "    for it in all_items:\n",
    "        se_orig = ST_e.get(it, 0.0) + SnotT_e.get(it, 0.0)\n",
    "        if se_orig >= abs_minsup:\n",
    "            L1_extra_output_only.add(frozenset([it]))\n",
    "\n",
    "    # Use only DT-frequent items to seed joining (keeps C2 manageable)\n",
    "    levels = [L1_DT]\n",
    "    Se_DT = dict(Se1)\n",
    "\n",
    "    # --- k >= 2 on DT with exact pair upper bound; conservative for k>2 ---\n",
    "    while levels[-1]:\n",
    "        Ck = apriori_gen(levels[-1])\n",
    "        if not Ck:\n",
    "            break\n",
    "\n",
    "        Sek_DT = expected_support_k_on_DT(DT, Ck)\n",
    "\n",
    "        # Frequent on DT outright\n",
    "        survivors = {c for c, se in Sek_DT.items() if se >= abs_minsup}\n",
    "\n",
    "        # Infrequent on DT: check global UB (exact for pairs)\n",
    "        for X, se_dt in Sek_DT.items():\n",
    "            if se_dt >= abs_minsup:\n",
    "                continue\n",
    "            UB_pair = global_prune_upper_bound_pair(X, ST_e, SnotT_e, MT, MnotT, ST_e_AB=se_dt)\n",
    "            if UB_pair is None:\n",
    "                # For k>2 we keep conservative behavior: no global pruning here.\n",
    "                continue\n",
    "            if se_dt + UB_pair >= abs_minsup:\n",
    "                survivors.add(X)\n",
    "\n",
    "        Se_DT.update(Sek_DT)\n",
    "        levels.append(survivors)\n",
    "\n",
    "    # --- Patch-up on original probabilistic data for final exact Se ---\n",
    "    candidates = set().union(*levels) if levels else set()\n",
    "\n",
    "    # Ensure singletons frequent overall are included in FINAL OUTPUT\n",
    "    candidates |= L1_extra_output_only\n",
    "\n",
    "    Se_true = {c: 0.0 for c in candidates}\n",
    "    for tx in prob_txns:\n",
    "        txset = set(tx.keys())\n",
    "        for c in candidates:\n",
    "            if c.issubset(txset):\n",
    "                prod = 1.0\n",
    "                for it in c:\n",
    "                    prod *= tx[it]\n",
    "                Se_true[c] += prod\n",
    "\n",
    "    F_final = {c for c, se in Se_true.items() if se >= abs_minsup}\n",
    "    secs = time.time() - t0\n",
    "    return F_final, Se_true, secs, rho_t, DT\n",
    "\n",
    "\n",
    "\n",
    "F_lgs, Se_lgs, secs_lgs, rho_t, DT = lgs_trimming(prob_txns, ABS_MINSUP)\n",
    "print(f\"LGS-Trimming: |F|={len(F_lgs)} | time={secs_lgs:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b5f60-68d3-4efd-a8cc-b7170695908a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 Head-to-head summary + top-k itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe443c8-3b07-4cbf-9893-7e29009be1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n=== Runtime (seconds) ===\")\n",
    "print(f\"U-Apriori     : {secs_u:.2f}\")\n",
    "print(f\"LGS-Trimming  : {secs_lgs:.2f}\")\n",
    "\n",
    "print(\"\\n=== # Frequent Itemsets ===\")\n",
    "print(f\"U-Apriori |F| : {len(F_u)}\")\n",
    "print(f\"LGS      |F|  : {len(F_lgs)}\")\n",
    "\n",
    "def top_itemsets(Se_map, F, k=TOPK_SHOW):\n",
    "    rows = sorted([(tuple(sorted(fs)), Se_map.get(fs, 0.0)) for fs in F], key=lambda x: -x[1])[:k]\n",
    "    return pd.DataFrame(rows, columns=[\"itemset\",\"expected_support\"])\n",
    "\n",
    "print(\"\\nTop-k itemsets by expected support (U-Apriori):\")\n",
    "display(top_itemsets(Se_u, F_u, k=15))\n",
    "\n",
    "print(\"\\nTop-k itemsets by expected support (LGS-Trimming):\")\n",
    "display(top_itemsets(Se_lgs, F_lgs, k=15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
